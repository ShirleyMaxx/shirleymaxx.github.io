
<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html lang="en"><head><meta name="generator" content="HTML Tidy for Linux/x86 (vers 11 February 2007), see www.w3.org">
  <link rel="stylesheet" type="text/css" href="stylesheet.css">
  <link rel="icon" type="image/png" href="seal_icon.png">
  <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.7.0/css/font-awesome.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons/css/academicons.min.css"/>
  <script type="text/javascript" src="js/hidebib.js"></script>
  <title>Xiaoxuan Ma</title>
  <meta name="Xiaoxuan Ma's Homepage"http-equiv="Content-Type" content="Xiaoxuan Ma's Homepage">

  <link href='https://fonts.googleapis.com/css?family=Lato:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
  <script src='https://www.google.com/recaptcha/api.js'></script>
<script>
    (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]//function(){
    (i[r].q=i[r].q//[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
    m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
    })(window,document,'script','//www.google-analytics.com/analytics.js','ga');
    ga('create', 'UA-41102261-2', 'auto');
    ga('send', 'pageview');
</script>
<style type="text/css">
#footer{padding:0 0 0.5em;margin-top:2rem;min-width:240px;font-size:12px;line-height:10px;text-align:center;}
#footer{:background-size: 15px 15px; background-repeat: repeat;}
#footer .container{max-width:540px;padding:0 45px;position:relative}
#footer .row{align-items:center}
#footer .copyright{flex-grow:1;margin:25px auto 5px;text-align:right;display:block;font-size:0.8rem;color:var(--main-color)}
#footer .stat{width:60px;height:60px;overflow:hidden;margin:0 auto 5px;padding 0;}
@media (min-width: 900px){ #footer .stat{display:block;width:60px;height:60px;overflow:hidden;margin:5px auto -25px;padding-right:15px;} }
</style>
<script>
  function toggleImage() {
    var img = document.getElementById('profileImage');
    if (img.src.includes('mxx_sea.jpg')) {
      img.src = 'images/me/mxx.png';
    } else {
      img.src = 'images/me/mxx_sea.jpg';
    }
  }
</script>
</head>

<body>
  <div class="fixed-block">
    <h2 style="margin-left: 5%; margin-top: 1%; display: inline-block;">Xiaoxuan Ma</h2>
    <p style="margin-left: 62%; display: inline-block; font-size: 20px">
      <a href="mailto:maxiaoxuan@pku.edu.cn" class="anocolor" style="font-weight: bold;"><i class="fa fa-envelope">&nbsp; </i></a> &nbsp; &nbsp; &nbsp;
      <a href="XiaoxuanMa_CV.pdf" target="_blank" class="anocolor" style="font-weight: bold;"><i class="ai ai-cv">&nbsp; </i></a> &nbsp; &nbsp; &nbsp;
      <a href="https://scholar.google.com/citations?user=mjP_5SEAAAAJ&hl=en" target="_blank" class="anocolor" style="font-weight: bold;"><i class="ai ai-google-scholar fa-xl">&nbsp; </i></a> &nbsp; &nbsp; &nbsp; 
      <a href="https://github.com/ShirleyMaxx" target="_blank" class="anocolor" style="font-weight: bold;"><i class="fa fa-github">&nbsp; </i></a> &nbsp; &nbsp; &nbsp;
      <a href="https://x.com/XiaoxuanMa_" target="_blank" class="anocolor" style="font-weight: bold;"><i class="fa fa-twitter">&nbsp; </i></a> &nbsp; &nbsp; &nbsp;
      <a href="https://www.linkedin.com/in/xiaoxuan-ma-740b7b122/" target="_blank" class="anocolor" style="font-weight: bold;"><i class="fa fa-linkedin">&nbsp; </i></a>
      
    </p>
  </div>
  <table style="width:100%;max-width:1000px;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
    <tr style="padding:0px">
      <td style="padding:0px">
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;"><tbody>
          <tr style="padding:0px">
            <td style="padding:2.5%;width:63%;vertical-align:middle">
              <p style="text-align:center; font-size: 22px !important;">
                <name>Xiaoxuan Ma 马霄璇</name>
              </p>
              <p>I'm a final year Ph.D. Candidate at <a href="https://cfcs.pku.edu.cn/english/", target="_blank" class="anocolor">CFCS</a>, School of Computer Science, Peking University. I'm a member of the <a href="https://cfcs.pku.edu.cn/english/research/researchlabs/237028.htm", target="_blank" style="font-weight: bold;">Computer Vision and Digital Art group</a>, advised by Prof. <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" style="font-weight: bold;">Yizhou Wang</a>. 
              I received my Bachelor's and Master's degrees in Computer Science from Peking University in 2018 and 2021, respectively.
              <br>
              <br>
              <p style="text-align:center">
                <i class="fa fa-envelope">&nbsp; </i><a href="mailto:maxiaoxuan@pku.edu.cn" style="font-weight: bold;">Email</a> &nbsp; &nbsp; &nbsp; 
                <i class="ai ai-google-scholar fa-xl">&nbsp; </i><a href="https://scholar.google.com/citations?user=mjP_5SEAAAAJ&hl=en", target="_blank" style="font-weight: bold;">Google Scholar</a>  &nbsp; &nbsp; &nbsp;
                <i class="fa fa-github">&nbsp; </i><a href="https://github.com/ShirleyMaxx", target="_blank" style="font-weight: bold;">Github</a> &nbsp; &nbsp; &nbsp; 
                <i class="fa fa-twitter">&nbsp; </i><a href="https://x.com/XiaoxuanMa_" target="_blank" style="font-weight: bold;">X</a> &nbsp; &nbsp; &nbsp;
                <i class="fa fa-linkedin">&nbsp; </i><a href="https://www.linkedin.com/in/xiaoxuan-ma-740b7b122/", target="_blank" style="font-weight: bold;">LinkedIn</a>
              </p>
            </td>
            <td style="padding:2.5%;width:25%;max-width:25%">
              <a href="javascript:void(0);" onclick="toggleImage();">
                <img id="profileImage" style="width:100%;max-width:100%" alt="profile photo" src="images/me/mxx_sea.jpg" class="hoverZoomLink">
              </a>
            </td>
          </tr>
        </tbody></table>

        
        <!-- Research -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Research</h2>
              My research interest lies at the intersection of computer vision, computer graphics, and machine learning, with a special focus on &#129302; &#128131;&#127995; <b>Human Digitization</b>. Specifically, I am dedicated to developing innovative algorithms for the reconstruction, animation, and generation of realistic 3D human avatars. These efforts have broad applications in AI-Generated Content (AIGC), as well as augmented and virtual reality (VR/AR). I am also passionate about applying computer vision techniques to scientific research, <i>i.e.</i> <b>CV for science</b>, to uncover new insights and drive innovation across various scientific domains. In particular, I am studying AI-assisted wildlife understanding, especially for non-human primates &#129421;, while promoting animal welfare. My ultimate research goal is to construct a dynamic <b>digital twin</b> of this 3D world, where humans, animals, and their environments are digitized, interacted and brought to life in a virtual realm.
              <br>
              <br>
              Fortunately, I have the privilege to collaborate with Principal Researcher Dr. <a href="https://www.chunyuwang.org/", target="_blank" style="font-weight: bold;">Chunyu Wang</a> from MSRA on the human digitalization work and spent wonderful time at <a href="https://www.microsoft.com/en-us/research/group/internet-media/", target="_blank" class="anocolor">MSRA IM Group</a>.
              In addition, I have the privilege to collaborate with Prof. <a href="https://yzhu.io/", target="_blank" style="font-weight: bold;">Yixin Zhu</a> from PKU and Prof. <a href="https://cogsci.ucsd.edu/people/faculty/federico-rossano.html", target="_blank" style="font-weight: bold;">Federico Rossano</a> from UCSD on the &#129421; AI-assisted wildlife understanding.

            <h3>Selected projects</h3>
              <p style="margin-left: 3%;">3D human modeling: &nbsp; <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_Context_Modeling_in_3D_Human_Pose_Estimation_A_Unified_Perspective_CVPR_2021_paper.pdf", target="_blank" style="font-weight: bold;">ContextPose</a>,  &nbsp; <a href="https://shirleymaxx.github.io/virtual_marker", target="_blank" style="font-weight: bold;">VirtualMarker</a>,  &nbsp; <a href="https://github.com/ShirleyMaxx/VMarker-Pro", target="_blank" style="font-weight: bold;">VMarker-Pro</a>,  &nbsp; <a href="https://xy02-05.github.io/ScoreHypo", target="_blank" style="font-weight: bold;">ScoreHypo</a> </p>
              <p style="margin-left: 3%;">CV for science, AI-assisted wildlife understanding: &nbsp; <img src="ChimpACT/images/icon.png" alt="Emoji Chimp" class="emoji-chimp" height="15px">&nbsp; <a href="https://shirleymaxx.github.io/ChimpACT/", target="_blank" style="font-weight: bold;">ChimpACT</a>,  &nbsp; <a href="https://sites.google.com/view/alphachimp/home", target="_blank" style="font-weight: bold;">AlphaChimp</a></p>
          </td></tr>
        </table>

        <br>
        <!-- Preprints -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">
          <h2 style="margin-left: 2%;">Preprints</h2>
          <h3 style="margin-left: 2%;">2024</h3>

          <tr>
            <td width="23%" valign="top"><a href="images/alphachimp.gif"><img src="images/alphachimp.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://sites.google.com/view/alphachimp/home", target="_blank" id="" class="anocolor">
              <heading>AlphaChimp: Tracking and Behavior Recognition of Chimpanzees</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
              <strong>Xiaoxuan Ma*</strong>,
              <a href="https://tongclass.ac.cn/author/yutang-lin/", target="_blank" class="anocolor">Yutang Lin*</a>,
              <a href="https://xy02-05.github.io/", target="_blank" class="anocolor">Yuan Xu</a>,
              <a href="https://carta.anthropogeny.org/users/stephan-kaufhold", target="_blank" class="anocolor">Stephan P. Kaufhold</a>,
              <a href="http://jackterwilliger.com/", target="_blank" class="anocolor">Jack Terwilliger</a>,
              <a href="https://www.linkedin.com/in/andy-meza-9bb064213/", target="_blank" class="anocolor">Andres Meza</a>,
              <a href="https://yzhu.io/", target="_blank" class="anocolor">Yixin Zhu</a>,
              <a href="https://cogsci.ucsd.edu/people/faculty/federico-rossano.html", target="_blank" class="anocolor">Federico Rossano</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>Under review</em>, 2024
              <br></p>
              <div class="paper" id="AlphaChimp">
                <a onmouseover="showblock('AlphaChimp_abs')" onmouseout="hideblock('AlphaChimp_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="" target="_blank" style="font-weight: bold;">Arxiv (coming soon) </a> |
                <a href="https://github.com/ShirleyMaxx/AlphaChimp", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://sites.google.com/view/alphachimp/home", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('AlphaChimp')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="AlphaChimp_abs">Understanding non-human primate behavior is crucial for improving animal welfare, modeling social behavior, and gaining insights into both distinctly human and shared behaviors. Despite recent advances in computer vision, automated analysis of primate behavior remains challenging due to the complexity of their social interactions and the lack of specialized algorithms. Existing methods often struggle with the nuanced behaviors and frequent occlusions characteristic of primate social dynamics. This study aims to develop an effective method for automated detection, tracking, and recognition of chimpanzee behaviors in video footage. Here we show that our proposed method, <b>AlphaChimp</b>, an end-to-end approach that simultaneously detects chimpanzee positions and estimates behavior categories from videos, significantly outperforms existing methods in behavior recognition. AlphaChimp achieves approximately 10% higher tracking accuracy and a 20% improvement in behavior recognition compared to state-of-the-art methods, particularly excelling in the recognition of social behaviors. This superior performance stems from AlphaChimp's innovative architecture, which integrates temporal feature fusion with a Transformer-based self-attention mechanism, enabling more effective capture and interpretation of complex social interactions among chimpanzees. Our approach bridges the gap between computer vision and primatology, enhancing technical capabilities and deepening our understanding of primate communication and sociality. We release our code and models and hope this will facilitate future research in animal social dynamics. This work contributes to ethology, cognitive science, and artificial intelligence, offering new perspectives on social intelligence.</i></p>
                <pre xml:space="preserve"><bibtex>@article{ma2024alphachimp,
    title={AlphaChimp: Tracking and Behavior Recognition of Chimpanzees},
    author={Ma, Xiaoxuan and Lin, Yutang and Xu, Yuan and Kaufhold, Stephan and Terwilliger, Jack and Meza, Andres and Zhu, Yixin and Rossano, Federico and Wang, Yizhou},
    journal={arXiv preprint arXiv},
    year={2024}
}</bibtex></pre>
              </div>
            </td>
          </tr> 


          <tr>
            <td width="23%" valign="top"><a href="images/vmarker_pro.gif"><img src="images/vmarker_pro.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/abs/2303.11726v3", target="_blank" id="" class="anocolor">
              <heading>VMarker-Pro: Probabilistic 3D Human Mesh Estimation from Virtual Markers</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank" class="anocolor">Jiajun Su</a>,
              <a href="https://xy02-05.github.io/", target="_blank" class="anocolor">Yuan Xu</a>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="https://www.chunyuwang.org/", target="_blank" class="anocolor">Chunyu Wang</a>, 
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>arXiv</em>, 2024
              <br></p>
              <div class="paper" id="VMPro">
                <a onmouseover="showblock('VMPro_abs')" onmouseout="hideblock('VMPro_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://arxiv.org/abs/2303.11726v3.pdf" target="_blank" style="font-weight: bold;">Arxiv</a> |
                <a href="https://github.com/ShirleyMaxx/VMarker-Pro", target="_blank" style="font-weight: bold;">Code</a> |
                <a shape="rect" href="javascript:togglebib('VMPro')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="VMPro_abs">Monocular 3D human mesh estimation faces challenges due to depth ambiguity and the complexity of mapping images to complex parameter spaces. Recent methods propose to use 3D poses as a proxy representation, which often lose crucial body shape information, leading to mediocre performance. Conversely, advanced motion capture systems, though accurate, are impractical for markerless wild images. Addressing these limitations, we introduce an innovative intermediate representation as <i>virtual markers</i>, which are learned from large-scale mocap data, mimicking the effects of physical markers. Building upon virtual markers, we propose VirtualMarker, which detects virtual markers from wild images, and the intact mesh with realistic shapes can be obtained by simply interpolation from these markers. To address occlusions that obscure 3D virtual marker estimation, we further enhance our method with VMarker-Pro, a probabilistic framework that generates multiple plausible meshes aligned with images. This framework models the 3D virtual marker estimation as a conditional denoising process, enabling robust 3D mesh estimation. Our approaches surpass existing methods on three benchmark datasets, particularly demonstrating significant improvements on the SURREAL dataset, which features diverse body shapes. Additionally, VMarker-Pro excels in accurately modeling data distributions, significantly enhancing performance in occluded scenarios.</i></p>
                <pre xml:space="preserve"><bibtex>@article{ma2024vmarkerpro,
    title={VMarker-Pro: Probabilistic 3D Human Mesh Estimation from Virtual Markers},
    author={Ma, Xiaoxuan and Su, Jiajun and Xu, Yuan and Zhu, Wentao and Wang, Chunyu and Wang, Yizhou},
    journal={arXiv preprint arXiv:2303.11726v3},
    year={2024}
}</bibtex></pre>
              </div>
            </td>
          </tr> 

          <tr>
            <td width="23%" valign="top"><a href="images/free_cloth.gif"><img src="images/free_cloth.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="", target="_blank" id="" class="anocolor">
              <heading>FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
              <a href="https://alvinyh.github.io/" target="_blank" class="anocolor">Hang Ye</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci</a>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>Under review</em>, 2024
              <br></p>
              <div class="paper" id="FreeCloth">
                <a onmouseover="showblock('FreeCloth_abs')" onmouseout="hideblock('FreeCloth_abs')" style="font-weight: bold;">Abstract</a> |
                <a shape="rect" href="javascript:togglebib('FreeCloth')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="FreeCloth_abs">Animating 3D clothed humans requires the modeling of pose-dependent deformations in various poses. The diversity of clothing styles and body poses makes this task extremely challenging.  Prior works rely on LBS skinning and predict local transformation in the canonical space. Nevertheless, it's quite challenging to model loose clothing, such as skirt and long dress, since they deviate significantly from the human body. Incorrect estimation of rigid transformations makes it difficult for the model to regress the residuals, which inevitably degrades the performance. In this work, we revisit the task of clothed humans modeling from a novel perspective. Our key insight is that canonicalization isn't a necessary step to predict clothing deformation, which circumvents the drawbacks of LBS posing in essence. Instead, we propose to learn features in the posed space. Without bell and whistle, our proposed model achieves state-of-the-arts performance on the CAPE and the ReSynth dataset. This simple yet effective paradigm provides a new possiblity of modeling pose-dependent deformation. Our model is capable of generating realistic 3D clothed humans with better perceptual quality.</i></p>
                <pre xml:space="preserve"><bibtex>@article{li2024efficient,
    title={FreeCloth: Free-form Generation Enhances Challenging Clothed Human Modeling},
    author={Ye, Hang and Ma, Xiaoxuan and Ci, Hai and Zhu, Wentao and Wang, Yizhou},
    journal={arXiv preprint},
    year={2024}
}</bibtex></pre>
              </div>
            </td>
          </tr> 

          <tr>
            <td width="23%" valign="top"><a href="images/detrc.gif"><img src="images/detrc.gif" width="75%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/abs/2403.01543v3", target="_blank" id="" class="anocolor">
              <heading>Efficient Action Counting with Dynamic Queries</b></heading></a>&nbsp; <img src="images/new.gif" alt="fast-texture" width="25" height="11">
              <br>
              <a target="_blank" class="anocolor">Zishi Li*</a>,
              <strong>Xiaoxuan Ma*</strong>,
              <a target="_blank" class="anocolor">Qiuyan Shang</a>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="http://www.pami.sjtu.edu.cn/yuqiao", target="_blank" class="anocolor">Yu Qiao</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>arXiv</em>, 2024
              <br></p>
              <div class="paper" id="DeTRC">
                <a onmouseover="showblock('DeTRC_abs')" onmouseout="hideblock('DeTRC_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://arxiv.org/abs/2403.01543v3.pdf" target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/lizishi/DeTRC", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://shirleymaxx.github.io/DeTRC/", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('DeTRC')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="DeTRC_abs">Temporal repetition counting aims to quantify the repeated action cycles within a video. The majority of existing methods rely on the similarity correlation matrix to characterize the repetitiveness of actions, but their scalability is hindered due to the quadratic computational complexity. In this work, we introduce a novel approach that employs an action query representation to localize repeated action cycles with linear computational complexity. Based on this representation, we further develop two key components to tackle the fundamental challenges of temporal repetition counting. Firstly, to facilitate open-set action counting, we propose the dynamic action query. Unlike static action queries, this approach dynamically embeds video features into action queries, offering a more flexible and generalizable representation. Second, to distinguish between actions of interest and background noise actions, we incorporate inter-query contrastive learning to regularize the video feature representation corresponding to different action queries. As a result, our method significantly outperforms previous works, particularly in terms of long video sequences, unseen actions, and actions at various speeds. On the challenging benchmark RepCountA, we outperform the state-of-the-art method TransRAC by 26.5% in OBO accuracy, with a 22.7% mean error decrease and 94.1% computational burden reduction.</i></p>
                <pre xml:space="preserve"><bibtex>@article{li2024efficient,
    title={Efficient Action Counting with Dynamic Queries},
    author={Li, Zishi and Ma, Xiaoxuan and Shang, Qiuyan and Zhu, Wentao and Ci, Hai and Qiao, Yu and Wang, Yizhou},
    journal={arXiv preprint arXiv:2403.01543v3},
    year={2024}
}</bibtex></pre>
              </div>
            </td>
          </tr> 

        </table>

        <!-- Publications -->
        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">
          <h2 style="margin-left: 2%;">Publications</h2>
          <h3 style="margin-left: 2%;">2024</h3>
          <tr>
            <td width="23%" valign="top"><a href="images/robotpose.gif"><img src="images/robotpose.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/abs/2402.05655", target="_blank" id="" class="anocolor">
              <heading>Real-time Holistic Robot Pose Estimation with Unknown States</b></heading></a>
              <br>
              <a href="https://github.com/Oliverbansk", target="_blank" class="anocolor">Shikun Ban</a>,
              <a target="_blank" class="anocolor">Juling Fan</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="http://www.pami.sjtu.edu.cn/yuqiao", target="_blank" class="anocolor">Yu Qiao</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>ECCV</em>, 2024
              <br></p>
              <div class="paper" id="RobotPose">
                <a onmouseover="showblock('RobotPose_abs')" onmouseout="hideblock('RobotPose_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://arxiv.org/abs/2402.05655.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/Oliverbansk/Hollistic-Robot-Pose-Estimation", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://oliverbansk.github.io/Holistic-Robot-Pose/", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('RobotPose')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="RobotPose_abs">Estimating robot pose from RGB images is a crucial problem in computer vision and robotics. While previous methods have achieved promising performance, most of them presume full knowledge of robot internal states, e.g. ground-truth robot joint angles, which are not always available in real-world scenarios. On the other hand, existing approaches that estimate robot pose without joint state priors suffer from heavy computation burdens and thus cannot support real-time applications. This work addresses the urgent need for efficient robot pose estimation with unknown states. We propose an end-to-end pipeline for real-time, holistic robot pose estimation from a single RGB image, even in the absence of known robot states. Our method decomposes the problem into estimating camera-to-robot rotation, robot state parameters, keypoint locations, and root depth. We further design a corresponding neural network module for each task. This approach allows for learning multi-facet representations and facilitates sim-to-real transfer through self-supervised learning. Notably, our method achieves inference with a single feedforward, eliminating the need for costly test-time iterative optimization. As a result, it delivers a 12 times speed boost with state-of-the-art accuracy, enabling real-time holistic robot pose estimation for the first time.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{ban2024real,
    title={Real-time Holistic Robot Pose Estimation with Unknown States},
    author={Ban, Shikun and Fan, Juling and Ma, Xiaoxuan and Zhu, Wentao and Qiao, Yu and Wang, Yizhou},
    booktitle={European Conference on Computer Vision},
    year={2024},
    organization={Springer}
}</bibtex></pre>
              </div>
            </td>
          </tr> 

          <tr>
            <td width="23%" valign="top"><a href="https://xy02-05.github.io/ScoreHypo/images/cvpr24_scorehypo.gif"><img src="https://xy02-05.github.io/ScoreHypo/images/cvpr24_scorehypo.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_ScoreHypo_Probabilistic_Human_Mesh_Estimation_with_Hypothesis_Scoring_CVPR_2024_paper.html", target="_blank" id="" class="anocolor">
              <heading>ScoreHypo: Probabilistic Human Mesh Estimation with Hypothesis Scoring</b></heading></a>
              <br>
              <a href="https://xy02-05.github.io/", target="_blank" class="anocolor">Yuan Xu</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank" class="anocolor">Jiajun Su</a>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="http://www.pami.sjtu.edu.cn/yuqiao", target="_blank" class="anocolor">Yu Qiao</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>CVPR</em>, 2024
              <br></p>
              <div class="paper" id="ScoreHypo">
                <a onmouseover="showblock('ScoreHypo_abs')" onmouseout="hideblock('ScoreHypo_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Xu_ScoreHypo_Probabilistic_Human_Mesh_Estimation_with_Hypothesis_Scoring_CVPR_2024_paper.html" target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/xy02-05/ScoreHypo", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://www.youtube.com/watch?v=LhHQ1ZlpKe0", target="_blank" style="font-weight: bold;">Video</a> |
                <a href="https://xy02-05.github.io/ScoreHypo", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('ScoreHypo')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="ScoreHypo_abs">Monocular 3D human mesh estimation is an ill-posed problem, characterized by inherent ambiguity and occlusion. While recent probabilistic methods propose generating multiple solutions, little attention is paid to obtaining high-quality estimates from them. To address this limitation, we introduce <b>ScoreHypo</b>, a versatile framework by first leveraging our novel <b>HypoNet</b> to generate multiple hypotheses, followed by employing a meticulously designed scorer, <b>ScoreNet</b>, to evaluate and select high-quality estimates. ScoreHypo formulates the estimation process as a reverse denoising process, where HypoNet produces a diverse set of plausible estimates that effectively align with the image cues. Subsequently, ScoreNet is employed to rigorously evaluate and rank these estimates based on their quality and finally identify superior ones. Experimental results demonstrate that HypoNet outperforms existing state-of-the-art probabilistic methods as a multi-hypothesis mesh estimator. Moreover, the estimates selected by ScoreNet significantly outperform random generation or simple averaging. Notably, the trained ScoreNet exhibits generalizability, as it can effectively score existing methods and significantly reduce their errors by more than 15%.</i></p>
                <pre xml:space="preserve"><bibtex>@InProceedings{Xu_2024_CVPR,
    author    = {Xu, Yuan and Ma, Xiaoxuan and Su, Jiajun and Zhu, Wentao and Qiao, Yu and Wang, Yizhou},
    title     = {ScoreHypo: Probabilistic Human Mesh Estimation with Hypothesis Scoring},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {979-989}
}</bibtex></pre>
              </div>
            </td>
          </tr> 

          <tr>
            <td width="23%" valign="top"><a href="images/cvpr24_trumans.gif"><img src="images/cvpr24_trumans.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/abs/2403.08629", target="_blank" id="" class="anocolor">
              <heading>Scaling Up Dynamic Human-Scene Interaction Modeling</b></heading></a>
              <br>
              <a href="https://jnnan.github.io/", target="_blank" class="anocolor">Nan Jiang*</a>,
              <a href="https://zhiyuan-zhang0206.github.io/", target="_blank" class="anocolor">Zhiyuan Zhang*</a>,
              <a href="https://pku.ai/author/hongjie-li/", target="_blank" class="anocolor">Hongjie Li</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://silvester.wang/", target="_blank" class="anocolor">Zan Wang</a>,
              <a href="https://yixchen.github.io/", target="_blank" class="anocolor">Yixin Chen</a>,
              <a href="https://tengyu.ai/", target="_blank" class="anocolor">Tengyu Liu</a>,
              <a href="https://yzhu.io/", target="_blank" class="anocolor">Yixin Zhu</a>,
              <a href="https://siyuanhuang.com/", target="_blank" class="anocolor">Siyuan Huang</a>
              <br>
              <em>CVPR</em>, 2024 <font color="#9900FF">(Highlight)</font>
              <br></p>
              <div class="paper" id="TRUMANS">
                <a onmouseover="showblock('TRUMANS_abs')" onmouseout="hideblock('TRUMANS_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2024/html/Jiang_Scaling_Up_Dynamic_Human-Scene_Interaction_Modeling_CVPR_2024_paper.html" target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://huggingface.co/spaces/jnnan/trumans/tree/main" target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://jnnan.github.io/trumans/", target="_blank" style="font-weight: bold;">Project page</a> |
                <a href="https://huggingface.co/spaces/jnnan/trumans", target="_blank" style="font-weight: bold;"> &#129303 Live demo</a> |
                <a shape="rect" href="javascript:togglebib('TRUMANS')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="TRUMANS_abs">The advancing of human-scene interaction modeling confronts substantial challenges in the scarcity of high-quality data and advanced motion synthesis methods. Previous endeavors have been inadequate in offering sophisticated datasets that effectively tackle the dual challenges of scalability and data quality. In this work, we overcome these challenges by introducing <b>TRUMANS</b> (TRacking hUMan ActioNs in Scenes), a large-scale MoCap dataset created by efficiently and precisely replicating the synthetic scenes in the physical environment. TRUMANS, featuring the most extensive motion-captured human-scene interaction datasets thus far, comprises over 15 hours of diverse human behaviors, including concurrent interactions with dynamic and articulated objects, across 100 indoor scene configurations. It provides accurate pose sequences of both humans and objects, ensuring a high level of contact plausibility during the interaction. To further enhance adaptivity, we propose a data augmentation approach that automatically adapts collision-free and interaction-precise human motions. Leveraging the benefits of TRUMANS, we propose a novel approach that employs a diffusion-based autoregressive mechanism for the real-time generation of human-scene interaction sequences with arbitrary length. The efficacy of TRUMANS and our motion synthesis method is validated through extensive experimental results, surpassing all existing baselines in terms of quality and diversity. Notably, our method demonstrates superb zero-shot generalizability on existing 3D scene datasets (e.g., PROX, Replica, ScanNet, ScanNet++), capable of generating even more realistic motions than the ground-truth annotations on PROX. Our human study further indicates that our generated motions are almost indistinguishable from the original motion-captured sequences, highlighting their superior quality. Our dataset and model will be released for research purposes.</i></p>
                <pre xml:space="preserve"><bibtex>@InProceedings{Jiang_2024_CVPR,
    author    = {Jiang, Nan and Zhang, Zhiyuan and Li, Hongjie and Ma, Xiaoxuan and Wang, Zan and Chen, Yixin and Liu, Tengyu and Zhu, Yixin and Huang, Siyuan},
    title     = {Scaling Up Dynamic Human-Scene Interaction Modeling},
    booktitle = {Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)},
    month     = {June},
    year      = {2024},
    pages     = {1737-1747}
}</bibtex></pre>
              </div>
            </td>
          </tr> 
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">
          <h3 style="margin-left: 2%;">2023</h3>
          <tr>
            <td width="23%" valign="top"><a href="images/nipsdb23_ChimpACT.gif"><img src="images/nipsdb23_ChimpACT.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/pdf/2310.16447.pdf", target="_blank" id="" class="anocolor">
              <heading><b><img src="ChimpACT/images/icon.png" alt="Emoji Chimp" class="emoji-chimp" height="15px">&nbsp; <span>ChimpACT</span>: A Longitudinal Dataset for Understanding Chimpanzee Behaviors</b></heading></a>
              <br>
              <strong>Xiaoxuan Ma*</strong>,
              <a href="https://carta.anthropogeny.org/users/stephan-kaufhold", target="_blank" class="anocolor">Stephan P. Kaufhold*</a>,
              <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank" class="anocolor">Jiajun Su*</a>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="http://jackterwilliger.com/", target="_blank" class="anocolor">Jack Terwilliger</a>,
              <a href="https://www.linkedin.com/in/andy-meza-9bb064213/", target="_blank" class="anocolor">Andres Meza</a>,
              <a href="https://yzhu.io/", target="_blank" class="anocolor">Yixin Zhu</a>,
              <a href="https://cogsci.ucsd.edu/people/faculty/federico-rossano.html", target="_blank" class="anocolor">Federico Rossano</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>NeurIPS</em>, 2023
              <br></p>
              <div class="paper" id="ChimpACT">
                <a onmouseover="showblock('ChimpACT_abs')" onmouseout="hideblock('ChimpACT_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://arxiv.org/pdf/2310.16447.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/ShirleyMaxx/ChimpACT", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://forms.gle/HDvxQonR676mog3d6", target="_blank" style="font-weight: bold;">Data</a> |
                <a href="https://vimeo.com/876025855", target="_blank" style="font-weight: bold;">Video</a> |
                <a href="https://shirleymaxx.github.io/ChimpACT/", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('ChimpACT')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="ChimpACT_abs">Understanding the behavior of non-human primates is crucial for improving animal welfare, modeling social behavior, and gaining insights into distinctively human and phylogenetically shared behaviors. However, the lack of datasets on non-human primate behavior hinders in-depth exploration of primate social interactions, posing challenges to research on our closest living relatives. To address these limitations, we present <b>ChimpACT</b>, a comprehensive dataset for quantifying the longitudinal behavior and social relations of chimpanzees within a social group. Spanning from 2015 to 2018, <b>ChimpACT</b> features videos of a group of over 20 chimpanzees residing at the Leipzig Zoo, Germany, with a particular focus on documenting the developmental trajectory of one young male, Azibo. <b>ChimpACT</b> is both comprehensive and challenging, consisting of 163 videos with a cumulative 160,500 frames, each richly annotated with detection, identification, pose estimation, and fine-grained spatiotemporal behavior labels. We benchmark representative methods of three tracks on <b>ChimpACT</b>: (i) tracking and identification, (ii) pose estimation, and (iii) spatiotemporal action detection of the chimpanzees. Our experiments reveal that <b>ChimpACT</b> offers ample opportunities for both devising new methods and adapting existing ones to solve fundamental computer vision tasks applied to chimpanzee groups, such as detection, pose estimation, and behavior analysis, ultimately deepening our comprehension of communication and sociality in non-human primates.</i></p>
                <pre xml:space="preserve"><bibtex>@article{ma2023chimpact,
  title={Chimpact: A longitudinal dataset for understanding chimpanzee behaviors},
  author={Ma, Xiaoxuan and Kaufhold, Stephan and Su, Jiajun and Zhu, Wentao and Terwilliger, Jack and Meza, Andres and Zhu, Yixin and Rossano, Federico and Wang, Yizhou},
  journal={Advances in Neural Information Processing Systems},
  volume={36},
  pages={27501--27531},
  year={2023}
}</bibtex></pre>
              </div>
            </td>
          </tr>  

          <tr>
            <td width="23%" valign="top"><a href="https://walter0807.github.io/Social-CH/assets/teaser.gif"><img src="https://walter0807.github.io/Social-CH/assets/teaser.gif" width="75%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/pdf/2311.04726.pdf", target="_blank" id="" class="anocolor">
              <heading>Social Motion Prediction with Cognitive Hierarchies</b></heading></a>
              <br>
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu*</a>,
              <a href="https://asonin.github.io/2022/09/23/intro/", target="_blank" class="anocolor">Jason Qin*</a>,
              <a href="https://thorin666.github.io/", target="_blank" class="anocolor">Yuke Lou</a>,
              <a href="https://alvinyh.github.io/", target="_blank" class="anocolor">Hang Ye</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>NeurIPS</em>, 2023
              <br></p>
              <div class="paper" id="SocialMotion">
                <a onmouseover="showblock('SocialMotion_abs')" onmouseout="hideblock('SocialMotion_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://arxiv.org/pdf/2311.04726.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/Walter0807/Social-CH", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://github.com/Walter0807/Social-CH/tree/main/data", target="_blank" style="font-weight: bold;">Data</a> |
                <a href="https://www.youtube.com/watch?v=pVBICYpGhyU", target="_blank" style="font-weight: bold;">Video</a> |
                <a href="https://walter0807.github.io/Social-CH/", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('SocialMotion')" class="togglebib" style="font-weight: bold;">Bibtex</a>
                    
                <p align="justify"> <i id="SocialMotion_abs">Humans exhibit a remarkable capacity for anticipating the actions of others and planning their own actions accordingly. In this study, we strive to replicate this ability by addressing the social motion prediction problem. We introduce a new benchmark, a novel formulation, and a cognition-inspired framework. We present <b>Wusi</b>, a 3D multi-person motion dataset under the context of team sports, which features intense and strategic human interactions and diverse pose distributions. By reformulating the problem from a multi-agent reinforcement learning perspective, we incorporate behavioral cloning and generative adversarial imitation learning to boost learning efficiency and generalization. Furthermore, we take into account the cognitive aspects of the human social action planning process and develop a cognitive hierarchy framework to predict strategic human social interactions. We conduct comprehensive experiments to validate the effectiveness of our proposed dataset and approach.</i></p>
                <pre xml:space="preserve"><bibtex>@article{zhu2024social,
    title={Social Motion Prediction with Cognitive Hierarchies},
    author={Zhu, Wentao and Qin, Jason and Lou, Yuke and Ye, Hang and Ma, Xiaoxuan and Ci, Hai and Wang, Yizhou},
    journal={Advances in Neural Information Processing Systems},
    volume={36},
    year={2024}
}</bibtex></pre>
              </div>
            </td>
          </tr>  

          <tr>
            <td width="23%" valign="top"><a href="images/motion_generation_survey.png"><img src="images/motion_generation_survey.png" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://arxiv.org/pdf/2307.10894.pdf", target="_blank" id="" class="anocolor">
              <heading>Human Motion Generation: A Survey</heading></a>
              <br>
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu*</a>,
              <strong>Xiaoxuan Ma*</strong>,
              <a href="https://github.com/dwro0121", target="_blank" class="anocolor">Dongwoo Ro*</a>,
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci</a>,
              <a href="https://jinluzhang.site/", target="_blank" class="anocolor">Jinlu Zhang</a>,
              <a href="https://shijx12.github.io/", target="_blank" class="anocolor">Jiaxin Shi</a>,
              <a href="https://www.art.pku.edu.cn/szdw/qzjs/ysglx/gf/index.htm", target="_blank" class="anocolor">Feng Gao</a>,
              <a href="https://www.qitian1987.com/", target="_blank" class="anocolor">Qi Tian</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>IEEE TPAMI</em>, 2023
              <br></p>
              <div class="paper" id="MotionGenerationSurvey">
                <a onmouseover="showblock('MotionGenerationSurvey_abs')" onmouseout="hideblock('MotionGenerationSurvey_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://arxiv.org/pdf/2307.10894.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a shape="rect" href="javascript:togglebib('MotionGenerationSurvey')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="MotionGenerationSurvey_abs">Human motion generation aims to generate natural human pose sequences and shows immense potential for real-world applications. Substantial progress has been made recently in motion data collection technologies and generation methods, laying the foundation for increasing interest in human motion generation. Most research within this field focuses on generating human motions based on conditional signals, such as text, audio, and scene contexts. While significant advancements have been made in recent years, the task continues to pose challenges due to the intricate nature of human motion and its implicit relationship with conditional signals. In this survey, we present a comprehensive literature review of human motion generation, which, to the best of our knowledge, is the first of its kind in this field. We begin by introducing the background of human motion and generative models, followed by an examination of representative methods for three mainstream sub-tasks: text-conditioned, audio-conditioned, and scene-conditioned human motion generation. Additionally, we provide an overview of common datasets and evaluation metrics. Lastly, we discuss open problems and outline potential future research directions. We hope that this survey could provide the community with a comprehensive glimpse of this rapidly evolving field and inspire novel ideas that address the outstanding challenges.</i></p>
                <pre xml:space="preserve"><bibtex>@article{zhu2024human,
  author={Zhu, Wentao and Ma, Xiaoxuan and Ro, Dongwoo and Ci, Hai and Zhang, Jinlu and Shi, Jiaxin and Gao, Feng and Tian, Qi and Wang, Yizhou},
  journal={IEEE Transactions on Pattern Analysis and Machine Intelligence}, 
  title={Human Motion Generation: A Survey}, 
  year={2024},
  volume={46},
  number={4},
  pages={2430-2449}
}</bibtex></pre>
              </div>
            </td>
          </tr>  

          <tr>
            <td width="23%" valign="top"><a href="https://motionbert.github.io/assets/teaser.gif"><img src="https://motionbert.github.io/assets/teaser.gif" width="75%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_MotionBERT_A_Unified_Perspective_on_Learning_Human_Motion_Representations_ICCV_2023_paper.pdf", target="_blank" id="" class="anocolor">
              <heading>MotionBERT: A Unified Perspective on Learning Human Motion Representations</heading></a>
              <br>
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://scholar.google.com/citations?user=btgwZosAAAAJ&hl=en", target="_blank" class="anocolor">Zhaoyang Liu</a>, 
              <a href="https://libliu.info/", target="_blank" class="anocolor">Libin Liu</a>, 
              <a href="https://wywu.github.io/", target="_blank" class="anocolor">Wayne Wu</a>, 
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>ICCV</em>, 2023
              <br></p>
              <div class="paper" id="MotionBERT">
                <a onmouseover="showblock('MotionBERT_abs')" onmouseout="hideblock('MotionBERT_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content/ICCV2023/papers/Zhu_MotionBERT_A_Unified_Perspective_on_Learning_Human_Motion_Representations_ICCV_2023_paper.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://arxiv.org/abs/2210.06551.pdf", target="_blank" style="font-weight: bold;">Arxiv</a> |
                <a href="https://github.com/Walter0807/MotionBERT/", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://motionbert.github.io/", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('MotionBERT')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="MotionBERT_abs">We present a unified perspective on tackling various human-centric video tasks by learning human motion representations from large-scale and heterogeneous data resources. Specifically, we propose a pretraining stage in which a motion encoder is trained to recover the underlying 3D motion from noisy partial 2D observations. The motion representations acquired in this way incorporate geometric, kinematic, and physical knowledge about human motion, which can be easily transferred to multiple downstream tasks. We implement the motion encoder with a Dual-stream Spatio-temporal Transformer (DSTformer) neural network. It could capture long-range spatio-temporal relationships among the skeletal joints comprehensively and adaptively, exemplified by the lowest 3D pose estimation error so far when trained from scratch. Furthermore, our proposed framework achieves state-of-the-art performance on all three downstream tasks by simply finetuning the pretrained motion encoder with a simple regression head (1-2 layers), which demonstrates the versatility of the learned motion representations.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{zhu2023motionbert,
    title={Motionbert: A unified perspective on learning human motion representations},
    author={Zhu, Wentao and Ma, Xiaoxuan and Liu, Zhaoyang and Liu, Libin and Wu, Wayne and Wang, Yizhou},
    booktitle={Proceedings of the IEEE/CVF International Conference on Computer Vision},
    pages={15085--15099},
    year={2023}
}</bibtex></pre>
              </div>
            </td>
          </tr>  
          
          <tr>
            <td width="23%" valign="top"><a href="images/cvpr23_virtualmarker.gif"><img src="images/cvpr23_virtualmarker.gif" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.pdf", target="_blank" id="" class="anocolor">
              <heading>3D Human Mesh Estimation from Virtual Markers</heading></a>
              <br>
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank" class="anocolor">Jiajun Su</a>,
              <a href="https://www.chunyuwang.org/", target="_blank" class="anocolor">Chunyu Wang</a>, 
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>CVPR</em>, 2023
              <br></p>
              <div class="paper" id="VirtualMarker">
                <a onmouseover="showblock('VirtualMarker_abs')" onmouseout="hideblock('VirtualMarker_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ma_3D_Human_Mesh_Estimation_From_Virtual_Markers_CVPR_2023_paper.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ma_3D_Human_Mesh_CVPR_2023_supplemental.pdf", target="_blank" style="font-weight: bold;">Supplementary</a> |
                <a href="https://github.com/ShirleyMaxx/VirtualMarker", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://shirleymaxx.github.io/virtual_marker", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('VirtualMarker')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="VirtualMarker_abs">Inspired by the success of volumetric 3D pose estimation, some recent human mesh estimators propose to estimate 3D skeletons as intermediate representations, from which, the dense 3D meshes are regressed by exploiting the mesh topology. However, body shape information is lost in extracting skeletons, leading to mediocre performance. The advanced motion capture systems solve the problem by placing dense physical markers on the body surface, which allows to extract realistic meshes from their non-rigid motions. However, they cannot be applied to wild images without markers. In this work, we present an intermediate representation, named virtual markers, which learns 64 landmark keypoints on the body surface based on the large-scale mocap data in a generative style, mimicking the effects of physical markers. The virtual markers can be accurately detected from wild images and can reconstruct the intact meshes with realistic shapes by simple interpolation. Our approach outperforms the state-of-the-art methods on three datasets. In particular, it surpasses the existing methods by a notable margin on the SURREAL dataset, which has diverse body shapes.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{ma20233d,
    title={3D Human Mesh Estimation from Virtual Markers},
    author={Ma, Xiaoxuan and Su, Jiajun and Wang, Chunyu and Zhu, Wentao and Wang, Yizhou},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={534--543},
    year={2023}
}</bibtex></pre>
              </div>
            </td>
          </tr>  

          <tr>
            <td width="23%" valign="top"><a href="images/cvpr23_gfpose.png"><img src="images/cvpr23_gfpose.png" width="90%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf", target="_blank" id="" class="anocolor">
              <heading>GFPose: Learning 3D Human Pose Prior with Gradient Fields</heading></a>
              <br>
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci</a>,
              <a href="https://aaronanima.github.io/", target="_blank" class="anocolor">Mingdong Wu</a>,
              <a href="https://wentao.live/", target="_blank" class="anocolor">Wentao Zhu</a>,
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://zsdonghao.github.io/", target="_blank" class="anocolor">Hao Dong</a>,
              <a href="https://fangweizhong.xyz/", target="_blank" class="anocolor">Fangwei Zhong</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>CVPR</em>, 2023
              <br></p>
              <div class="paper" id="GFPose">
                <a onmouseover="showblock('GFPose_abs')" onmouseout="hideblock('GFPose_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2023/papers/Ci_GFPose_Learning_3D_Human_Pose_Prior_With_Gradient_Fields_CVPR_2023_paper.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2023/supplemental/Ci_GFPose_Learning_3D_CVPR_2023_supplemental.pdf", target="_blank" style="font-weight: bold;">Supplementary</a> |
                <a href="https://github.com/Embracing/GFPose", target="_blank" style="font-weight: bold;">Code</a> |
                <a href="https://sites.google.com/view/gfpose/home", target="_blank" style="font-weight: bold;">Project page</a> |
                <a shape="rect" href="javascript:togglebib('GFPose')" class="togglebib" style="font-weight: bold;">Bibtex</a> 
		      
                <p align="justify"> <i id="GFPose_abs">Learning 3D human pose prior is essential to human-centered AI. Here, we present GFPose, a versatile framework to model plausible 3D human poses for various applications. At the core of GFPose is a time-dependent score network, which estimates the gradient on each body joint and progressively denoises the perturbed 3D human pose to match a given task specification. During the denoising process, GFPose implicitly incorporates pose priors in gradients and unifies various discriminative and generative tasks in an elegant framework. Despite the simplicity, GFPose demonstrates great potential in several downstream tasks. Our experiments empirically show that 1) as a multi-hypothesis pose estimator, GFPose outperforms existing SOTAs by 20% on Human3.6M dataset. 2) as a single-hypothesis pose estimator, GFPose achieves comparable results to deterministic SOTAs, even with a vanilla backbone. 3) GFPose is able to produce diverse and realistic samples in pose denoising, completion and generation tasks.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{ci2023gfpose,
    title={Gfpose: Learning 3d human pose prior with gradient fields},
    author={Ci, Hai and Wu, Mingdong and Zhu, Wentao and Ma, Xiaoxuan and Dong, Hao and Zhong, Fangwei and Wang, Yizhou},
    booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
    pages={4800--4810},
    year={2023}
}</bibtex></pre>
              </div>
            </td>
          </tr>  
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">
          <h3 style="margin-left: 2%;">2022</h3>
          <tr>
            <td width="23%" valign="top"><a href="images/tpami22_lcn_pami.png"><img src="images/tpami22_lcn_pami.png" width="60%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://ieeexplore.ieee.org/document/9174911", target="_blank" id="" class="anocolor">
              <heading>Locally Connected Network for Monocular 3D Human Pose Estimation</heading></a>
              <br>
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci*</a>,
              <strong>Xiaoxuan Ma*</strong>,
              <a href="https://www.chunyuwang.org/", target="_blank" class="anocolor">Chunyu Wang</a>, 
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>IEEE TPAMI</em>, 2022
              <br></p>
              <div class="paper" id="LCN_PAMI">
                <a onmouseover="showblock('LCN_PAMI_abs')" onmouseout="hideblock('LCN_PAMI_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://ieeexplore.ieee.org/document/9174911", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/CHUNYUWANG/lcn-pose", target="_blank" style="font-weight: bold;">Code</a> |
                <a shape="rect" href="javascript:togglebib('LCN_PAMI')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="LCN_PAMI_abs">We present an approach for 3D human pose estimation from monocular images. The approach consists of two steps: it first estimates a 2D pose from an image and then estimates the corresponding 3D pose. This paper focuses on the second step. Graph convolutional network (GCN) has recently become the de facto standard for human pose related tasks such as action recognition. However, in this work, we show that GCN has critical limitations when it is used for 3D pose estimation due to the inherent weight sharing scheme. The limitations are clearly exposed through a novel reformulation of GCN, in which both GCN and Fully Connected Network (FCN) are its special cases. In addition, on top of the formulation, we present locally connected network (LCN) to overcome the limitations of GCN by allocating dedicated rather than shared filters for different joints. We jointly train the LCN network with a 2D pose estimator such that it can handle inaccurate 2D poses. We evaluate our approach on two benchmark datasets and observe that LCN outperforms GCN, FCN, and the state-of-the-art methods by a large margin. More importantly, it demonstrates strong cross-dataset generalization ability because of sparse connections among body joints.</i></p>
                <pre xml:space="preserve"><bibtex>@article{ci2020locally,
    title={Locally connected network for monocular 3D human pose estimation},
    author={Ci, Hai and Ma, Xiaoxuan and Wang, Chunyu and Wang, Yizhou},
    journal={IEEE Transactions on Pattern Analysis and Machine Intelligence},
    volume={44},
    number={3},
    pages={1429--1442},
    year={2020},
    publisher={IEEE}
}</bibtex></pre>
              </div>
            </td>
          </tr> 
          
          <tr>
            <td width="23%" valign="top"><a href="images/eccv22_virturalpose.png"><img src="images/eccv22_virturalpose.png" width="70%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660054.pdf", target="_blank" id="" class="anocolor">
              <heading>Virtual Pose: Learning Generalizable 3D Human Pose Models from Virtual Data</heading></a>
              <br>
              <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank" class="anocolor">Jiajun Su</a>,
              <a href="https://www.chunyuwang.org/", target="_blank" class="anocolor">Chunyu Wang</a>, 
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://scholar.google.com/citations?user=_cUfvYQAAAAJ&hl=zh-CN", target="_blank" class="anocolor">Wenjun Zeng</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>ECCV</em>, 2022
              <br></p>
              <div class="paper" id="VirtualPose">
                <a onmouseover="showblock('VirtualPose_abs')" onmouseout="hideblock('VirtualPose_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://www.ecva.net/papers/eccv_2022/papers_ECCV/papers/136660054.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/wkom/VirtualPose", target="_blank" style="font-weight: bold;">Code</a> |
                <a shape="rect" href="javascript:togglebib('VirtualPose')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="VirtualPose_abs">While monocular 3D pose estimation seems to have achieved very accurate results on the public datasets, their generalization ability is largely overlooked. In this work, we perform a systematic evaluation of the existing methods and find that they get notably larger errors when tested on different cameras, human poses and appearance. To address the problem, we introduce VirtualPose, a two-stage learning framework to exploit the hidden “free lunch” specific to this task, i.e.generating infinite number of poses and cameras for training models at no cost. To that end, the first stage transforms images to abstract geometry representations (AGR), and then the second maps them to 3D poses. It addresses the generalization issue from two aspects: (1) the first stage can be trained on diverse 2D datasets to reduce the risk of over-fitting to limited appearance; (2) the second stage can be trained on diverse AGR synthesized from a large number of virtual cameras and poses. It outperforms the SOTA methods without using any paired images and 3D poses from the benchmarks, which paves the way for practical applications.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{su2022virtualpose,
    title={Virtualpose: Learning generalizable 3d human pose models from virtual data},
    author={Su, Jiajun and Wang, Chunyu and Ma, Xiaoxuan and Zeng, Wenjun and Wang, Yizhou},
    booktitle={European Conference on Computer Vision},
    pages={55--71},
    year={2022},
    organization={Springer}
}</bibtex></pre>
              </div>
            </td>
          </tr>  
        </table>

        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">
          <h3 style="margin-left: 2%;">2021</h3>
          <tr>
            <td width="23%" valign="top"><a href="images/cvpr21_contextpose.gif"><img src="images/cvpr21_contextpose.gif" width="60%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_Context_Modeling_in_3D_Human_Pose_Estimation_A_Unified_Perspective_CVPR_2021_paper.pdf", target="_blank" id="" class="anocolor">
              <heading>Context Modeling in 3D Human Pose Estimation: A Unified Perspective</heading></a>
              <br>
              <strong>Xiaoxuan Ma*</strong>,
              <a href="https://scholar.google.com/citations?user=DoUvUz4AAAAJ&hl=zh-CN", target="_blank" class="anocolor">Jiajun Su*</a>,
              <a href="https://www.chunyuwang.org/", target="_blank" class="anocolor">Chunyu Wang</a>, 
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci</a>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>CVPR</em>, 2021
              <br></p>
              <div class="paper" id="ContextPose">
                <a onmouseover="showblock('ContextPose_abs')" onmouseout="hideblock('ContextPose_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content/CVPR2021/papers/Ma_Context_Modeling_in_3D_Human_Pose_Estimation_A_Unified_Perspective_CVPR_2021_paper.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/ShirleyMaxx/ContextPose-PyTorch-release", target="_blank" style="font-weight: bold;">Code</a> |
                <a shape="rect" href="javascript:togglebib('ContextPose')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="ContextPose_abs">Estimating 3D human pose from a single image suffers from severe ambiguity since multiple 3D joint configurations may have the same 2D projection. The state-of-the-art methods often rely on context modeling methods such as pictorial structure model (PSM) or graph neural network (GNN) to reduce ambiguity. However, there is no study that rigorously compares them side by side. So we first present a general formula for context modeling in which both PSM and GNN are its special cases. By comparing the two methods, we found that the end-to-end training scheme in GNN and the limb length constraints in PSM are two complementary factors to improve results. To combine their advantages, we propose ContextPose based on attention mechanism that allows enforcing soft limb length constraints in a deep network. The approach effectively reduces the chance of getting absurd 3D pose estimates with incorrect limb lengths and achieves state-of-the-art results on two benchmark datasets. More importantly, the introduction of limb length constraints into deep networks enables the approach to achieve much better generalization performance.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{ma2021context,
    title={Context modeling in 3d human pose estimation: A unified perspective},
    author={Ma, Xiaoxuan and Su, Jiajun and Wang, Chunyu and Ci, Hai and Wang, Yizhou},
    booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
    pages={6238--6247},
    year={2021}
}</bibtex></pre>
              </div>
            </td>
          </tr>  
        </table>


        <table style="width:100%;border:0px;border-spacing:0px;border-collapse:separate;margin-right:auto;margin-left:auto;" cellspacing="0" cellpadding="20">
          <h3 style="margin-left: 2%;">2019</h3>
          <tr>
            <td width="23%" valign="top"><a href="images/iccv19_lcn.gif"><img src="images/iccv19_lcn.gif" width="80%" style="border-style: none; display: block; margin: 0 auto;"></a>
            <td width="72%" valign="top">
              <p style="margin: 0;"><a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.pdf", target="_blank" id="" class="anocolor">
              <heading>Optimizing Network Structure for 3D Human Pose Estimation</heading></a>
              <br>
              <a href="https://haici.cc/", target="_blank" class="anocolor">Hai Ci</a>,
              <a href="https://www.chunyuwang.org/", target="_blank" class="anocolor">Chunyu Wang</a>, 
              <strong>Xiaoxuan Ma</strong>,
              <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor">Yizhou Wang</a>
              <br>
              <em>ICCV</em>, 2019
              <br></p>
              <div class="paper" id="LCN">
                <a onmouseover="showblock('LCN_abs')" onmouseout="hideblock('LCN_abs')" style="font-weight: bold;">Abstract</a> |
                <a href="https://openaccess.thecvf.com/content_ICCV_2019/papers/Ci_Optimizing_Network_Structure_for_3D_Human_Pose_Estimation_ICCV_2019_paper.pdf", target="_blank" style="font-weight: bold;">Paper</a> |
                <a href="https://github.com/CHUNYUWANG/lcn-pose", target="_blank" style="font-weight: bold;">Code</a> |
                <a shape="rect" href="javascript:togglebib('LCN')" class="togglebib" style="font-weight: bold;">Bibtex</a>
		      
                <p align="justify"> <i id="LCN_abs">A human pose is naturally represented as a graph where the joints are the nodes and the bones are the edges. So it is natural to apply Graph Convolutional Network (GCN) to estimate 3D poses from 2D poses. In this work, we propose a generic formulation where both GCN and Fully Connected Network (FCN) are its special cases. From this formulation, we discover that GCN has limited representation power when used for estimating 3D poses. We overcome the limitation by introducing Locally Connected Network (LCN) which is naturally implemented by this generic formulation. It notably improves the representation capability over GCN. In addition, since every joint is only connected to a few joints in its neighborhood, it has strong generalization power. The experiments on public datasets show it: (1) outperforms the state-of-the-arts; (2) is less data hungry than alternative models; (3) generalizes well to unseen actions and datasets.</i></p>
                <pre xml:space="preserve"><bibtex>@inproceedings{ci2019optimizing,
    title={Optimizing network structure for 3d human pose estimation},
    author={Ci, Hai and Wang, Chunyu and Ma, Xiaoxuan and Wang, Yizhou},
    booktitle={Proceedings of the IEEE/CVF international conference on computer vision},
    pages={2262--2271},
    year={2019}
}</bibtex></pre>
              </div>
            </td>
          </tr> 

        </table>


        <br>
        <br>
        <br>
        <!-- Services -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Academic Services</h2>
            <p style="margin-left: 3%; display: inline; font-weight: bold;">Journal Reviewer:</p>
            <p style="margin-left: 2%; display: inline;">IEEE TPAMI, IJCV, IEEE TMM</p>
            <br>
            <p style="margin-left: 3%; display: inline; font-weight: bold;">Conference Reviewer:</p>
            <p style="margin-left: 2%; display: inline;">CVPR, ICCV, ECCV, NeurIPS, ICLR, AAAI</p>
          </td></tr>
        </table>

        <br>
        <!-- Teaching -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Teaching</h2>
            <p style="margin-left: 3%;"><a href="https://idm.pku.edu.cn/info/1032/1230.htm", target="_blank" style="font-weight: bold;">Computational Vision</a> (TA) <span style="float: right;">Fall 2019</span></p>
            <p style="margin-left: 3%;"><a href="https://www.oir.pku.edu.cn/summerschool/info/1044/1121.htm", target="_blank" style="font-weight: bold;">Machine Learning for Time Series Analysis – Statistical Models and Deep Learning</a> (TA) <span style="float: right;">Summer 2018</span></p>
            <p style="margin-left: 3%;"><a href="https://www.oir.pku.edu.cn/summerschool/info/1044/1120.htm", target="_blank" style="font-weight: bold;">Computation, Economics and Data Science</a> (TA) <span style="float: right;">Summer 2018</span></p>
        </td></tr>
        </table>

        <br>
        <!-- Mentoring -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Co-mentoring</h2>
            <p>I am fortunate to have the opportunity to work with a few talented and highly motivated junior students (ordered by year and last name):</p>
            <div style="width: 100%; margin-left: 3%;">
                <div style="display: flex;">
                    <p style="width: 30%; margin: 0;">&middot; <a href="https://alvinyh.github.io/" target="_blank">Hang Ye</a> (2022-Now)</p>
                    <p style="width: 100%; margin: 0;">now second-year Ph.D. at PKU, advised by Prof. <a href="https://idm.pku.edu.cn/info/1017/1037.htm" target="_blank" class="anocolor" style="font-weight: bold;">Yizhou Wang</a></p>
                </div>
                <div style="display: flex;">
                    <p style="width: 30%; margin: 0;">&middot; <a href="https://xy02-05.github.io/", target="_blank">Yuan Xu</a> (2022-Now)</p>
                    <p style="width: 100%; margin: 0;">now first-year Ph.D. at PKU, advised by Prof. <a href="https://idm.pku.edu.cn/info/1017/1037.htm" target="_blank" class="anocolor" style="font-weight: bold;">Yizhou Wang</a></p>
                </div>
                <div style="display: flex;">
                    <p style="width: 30%; margin: 0;">&middot; Chi Su</a> (2023-Now)</p>
                    <p style="width: 100%; margin: 0;">senior undergraduate, incoming Ph.D. at PKU, advised by Prof. <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor" style="font-weight: bold;">Yizhou Wang</a></p>
                </div>
                <div style="display: flex;">
                    <p style="width: 30%; margin: 0;">&middot; <a href="https://github.com/Oliverbansk", target="_blank">Shikun Ban</a> (2022-Now)</p>
                    <p style="width: 100%; margin: 0;">undergraduate at PKU</p>
                </div>
                <div style="display: flex;">
                    <p style="width: 30%; margin: 0;">&middot; Juling Fan (2022-2024)</p>
                    <p style="width: 100%; margin: 0;">undergraduate at PKU</p>
                </div>
            </div>
        </td></tr>
        </table>


        
        <br>
        <!-- Education -->
        <table width="100%" align="center" border="0" cellspacing="0" cellpadding="20">
          <tr><td>
            <h2>Education</h2>
          </td></tr>
        </table>
        <table width="100%" align="center" border="0" cellpadding="20"><tbody>
          <tr>
            <td style="padding:10px;width:15%;vertical-align:middle">
              <img src="images/me/pku.png" alt="eth" width="36%" style="margin-left: 25%;">
            </td>
            <td width="75%" valign="center">
              <b>Ph.D. candidate</b>
              <br>
              <a href="https://cfcs.pku.edu.cn/english/research/researchlabs/237028.htm", target="_blank" class="anocolor">CVDA Lab, <a href="https://cfcs.pku.edu.cn/english/", target="_blank" class="anocolor">CFCS</a>, <a href="https://cs.pku.edu.cn/English/Home.htm", target="_blank" class="anocolor">School of Computer Science</a>, Peking University, China
              <br>
              Sep. 2021 ~ Now
              <br>
              Supervisor: Prof. <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor" style="font-weight: bold;">Yizhou Wang</a>
            </td>
          </tr>

          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/me/pku.png" alt="eth" width="36%" style="margin-left: 25%;">
            </td>
            <td width="75%" valign="center">
              <b>Master's degree</b>
              <br>
              <a href="https://cfcs.pku.edu.cn/english/research/researchlabs/237028.htm", target="_blank" class="anocolor">CVDA Lab, <a href="https://cfcs.pku.edu.cn/english/", target="_blank" class="anocolor">CFCS</a>, <a href="https://cs.pku.edu.cn/English/Home.htm", target="_blank" class="anocolor">School of Computer Science</a>, Peking University, China
              <br>
              Sep. 2018 ~ Jul. 2021
              <br>
              Supervisor: Prof. <a href="https://idm.pku.edu.cn/info/1017/1037.htm", target="_blank" class="anocolor" style="font-weight: bold;">Yizhou Wang</a>
            </td>
          </tr>
          
          <tr>
            <td style="padding:10px;width:25%;vertical-align:middle">
              <img src="images/me/pku.png" alt="eth" width="36%" style="margin-left: 25%;">
            </td>
            <td width="75%" valign="center">
              <b>Bachelor's degree</b>
              <br>
              <a href="https://eecs.pku.edu.cn/en/", target="_blank" class="anocolor">Depart. of Computer Science</a>, Peking University, China
              <br>
              Sep. 2014 ~ Jul. 2018
              <br>
            </td>
          </tr>
        </table>

        <br>
        <br>

        <div id="footer" style="width: 60%; margin-left: auto;">
          <div class="container">
              <div class="row">
                  <div class="copyright">
                    Template courtesy of <a href="https://jonbarron.info/", target="_blank">Jon Barron</a>. Last update: 10/2024.
                  </div>
                  <div class="stat">
                  </div>
              </div>
          </div>
        </div>




<script xml:space="preserve" language="JavaScript">
hideallbibs();
</script>

<script xml:space="preserve" language="JavaScript">
hideblock('AlphaChimp_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('VMPro_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('FreeCloth_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('RobotPose_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('DeTRC_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('TRUMANS_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ScoreHypo_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('SocialMotion_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ChimpACT_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('MotionGenerationSurvey_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('MotionBERT_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('VirtualMarker_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('GFPose_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('VirtualPose_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('ContextPose_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('LCN_PAMI_abs');
</script>
<script xml:space="preserve" language="JavaScript">
hideblock('LCN_abs');
</script>


<script>
    const bibTex_list = document.querySelectorAll("bibtex");

    bibTex_list.forEach((bibTex) => {
        bibTex.onclick = function() {
          document.execCommand("copy");
      }

      bibTex.addEventListener("copy", function(event) {
      event.preventDefault();
      if (event.clipboardData) {
          event.clipboardData.setData("text/plain", bibTex.innerText);
          alert("Citation copied to clipboard!");
      }
      });
    });

</script>

</body>

</html>
